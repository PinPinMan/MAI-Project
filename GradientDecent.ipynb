{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MAI Assignment**\n",
    "<hr>\n",
    "Name: Goh Pin Pin Isaac<br>\n",
    "Admin Number: P2317623<br>\n",
    "Class: DAAA/FT/2B/23<hr>\n",
    "\n",
    "# **Background**\n",
    "Raising children in today's world is more important than ever, as we are seeing a troubling rise in aggression among the younger generation. This increase can be linked to various factors such as exposure to violent media, societal pressures, and the fast-paced, stressful nature of modern life. It is crucial for parents, educators, and communities to provide a nurturing and supportive environment that promotes emotional well-being and positive behavior.\n",
    "\n",
    "By teaching values like empathy, patience, and respect, we can help children develop healthy ways to express themselves and manage their emotions. Ensuring that children have strong role models and the tools to navigate challenges will not only reduce aggression but also foster a more compassionate and harmonious society.\n",
    "\n",
    "Hence, a study was carried out to explore the relationship between aggression and several potential predicting factors in 666 children who had an older sibling.<br><br>\n",
    "[**Case Study & Dataset**](https://rdrr.io/github/profandyfield/discovr/man/child_aggression.html)\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "# **About Dataset**\n",
    "- **aggression**: The child's aggression (high score = more aggression seen)\n",
    "- **television**: Time spent watching television (high score = more time spent watching television)\n",
    "- **computer_games**: Time spent playing video games (high score = more time spent playing video games)\n",
    "- **sibling_aggression**: Aggression in older sibling (high score = more aggression seen in their older sibling)\n",
    "- **diet**: The child's diet (high score = the child has a good diet low in harmful additives)\n",
    "- **parenting_style**: the parent's parenting style (high score = bad parenting practices)\n",
    "<br><br>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libriaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# gradient descent\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "child_aggression = pd.read_csv('./data/child_aggression.csv')\n",
    "child_aggression.drop(['television', 'computer_games','diet'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 666 entries, 0 to 665\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   aggression          666 non-null    float64\n",
      " 1   sibling_aggression  666 non-null    float64\n",
      " 2   parenting_style     666 non-null    float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 15.7 KB\n"
     ]
    }
   ],
   "source": [
    "child_aggression.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aggression</th>\n",
       "      <td>666.0</td>\n",
       "      <td>-5.011186e-03</td>\n",
       "      <td>0.319404</td>\n",
       "      <td>-1.295608</td>\n",
       "      <td>-0.174279</td>\n",
       "      <td>-0.005548</td>\n",
       "      <td>0.149611</td>\n",
       "      <td>1.178823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sibling_aggression</th>\n",
       "      <td>666.0</td>\n",
       "      <td>8.274587e-03</td>\n",
       "      <td>0.326806</td>\n",
       "      <td>-1.433127</td>\n",
       "      <td>-0.156414</td>\n",
       "      <td>0.008459</td>\n",
       "      <td>0.185136</td>\n",
       "      <td>1.103671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parenting_style</th>\n",
       "      <td>666.0</td>\n",
       "      <td>-6.001206e-17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.460406</td>\n",
       "      <td>-0.580084</td>\n",
       "      <td>0.027364</td>\n",
       "      <td>0.517836</td>\n",
       "      <td>3.993256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count          mean       std       min       25%  \\\n",
       "aggression          666.0 -5.011186e-03  0.319404 -1.295608 -0.174279   \n",
       "sibling_aggression  666.0  8.274587e-03  0.326806 -1.433127 -0.156414   \n",
       "parenting_style     666.0 -6.001206e-17  1.000000 -4.460406 -0.580084   \n",
       "\n",
       "                         50%       75%       max  \n",
       "aggression         -0.005548  0.149611  1.178823  \n",
       "sibling_aggression  0.008459  0.185136  1.103671  \n",
       "parenting_style     0.027364  0.517836  3.993256  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_aggression.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$Error Function Formula$$\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{i=1}^{n} e_i^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\left[ (y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\cdots + (y_n - \\hat{y}_n)^2 \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL 1:  SLR with intercept a fixed** ($ùíöÃÇ_ùíä =ùíÉùíô_ùíä$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y data\n",
    "x_data = child_aggression['parenting_style'].values\n",
    "y_data = child_aggression['aggression'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the symbols\n",
    "b = sp.symbols('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.998498498498497 b^{2} - 0.13438726075923 b + 0.101890580668516$"
      ],
      "text/plain": [
       "0.998498498498497*b**2 - 0.13438726075923*b + 0.101890580668516"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the error function (Mean Squared Error)\n",
    "error_function = sum((y - b*x)**2 for x, y in zip(x_data, y_data))/len(x_data)\n",
    "error_function.simplify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$Error Function$$\n",
    "\n",
    "$$\n",
    "E(b) = \\frac{1}{666} \\left[ \\begin{array}{c}\n",
    "(y - bx_1)^2 + \\\\ \n",
    "(y - bx_2)^2 + \\\\ \n",
    "(y - bx_3)^2 + \\\\ \n",
    "\\vdots \\\\\n",
    "(y - bx_{664})^2 + \\\\ \n",
    "(y - bx_{665})^2 + \\\\ \n",
    "(y - bx_{666})^2\n",
    "\\end{array} \\right] \n",
    "$$\n",
    "\n",
    "### $$Substitute$$\n",
    "\n",
    "$$\n",
    "E(b) = \\frac{1}{666} \\left[ \\begin{array}{c}\n",
    "(0.374 - (-0.279b))^2 + \\\\ \n",
    "(0.771 - (-1.248b))^2 + \\\\ \n",
    "(-0.098 - (-0.328b))^2 + \\\\\n",
    "\\vdots \\\\\n",
    "(-0.062 - (-0.335b))^2 + \\\\\n",
    "(0.104 - (-0.475b))^2 + \\\\\n",
    "(-0.316 - (-1.102b))^2\n",
    "\\end{array} \\right] \n",
    "$$\n",
    "\n",
    "### $$Simplify$$\n",
    "\n",
    "$$\n",
    "E(b) = 0.998b^2‚àí0.134b+0.102\n",
    "$$\n",
    "\n",
    "### $$E'(b)$$\n",
    "$$\n",
    "E'(b) = 2b‚àí0.134\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $E(b)=b^2-0.134b+0.102$ is a **parabola** that opens upwards because the coefficient of 665 is positive. <br>\n",
    "A parabola that opens upwards has `exactly one minimum point`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables for gradient descent\n",
    "b_value = 0.0  # Starting value of b\n",
    "learning_rate = 0.15\n",
    "epsilon = 0.00001\n",
    "max_iter = 1000\n",
    "diff = 1\n",
    "iter_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: b-value is 0.0201580891138845\n",
      "Iteration 2: b-value is 0.03427783171392523\n",
      "Iteration 3: b-value is 0.044168011778368196\n",
      "Iteration 4: b-value is 0.05109559285954335\n",
      "Iteration 5: b-value is 0.05594802014838451\n",
      "Iteration 6: b-value is 0.05934690502863136\n",
      "Iteration 7: b-value is 0.0617276554740295\n",
      "Iteration 8: b-value is 0.06339525319591874\n",
      "Iteration 9: b-value is 0.06456332277138621\n",
      "Iteration 10: b-value is 0.06538149763167986\n",
      "Iteration 11: b-value is 0.06595458858111979\n",
      "Iteration 12: b-value is 0.06635601039480406\n",
      "Iteration 13: b-value is 0.06663718648501984\n",
      "Iteration 14: b-value is 0.06683413640406739\n",
      "Iteration 15: b-value is 0.06697209006358042\n",
      "Iteration 16: b-value is 0.0670687197665276\n",
      "Iteration 17: b-value is 0.06713640408548385\n",
      "Iteration 18: b-value is 0.0671838135971852\n",
      "Iteration 19: b-value is 0.06721702161101203\n",
      "Iteration 20: b-value is 0.06724028217925561\n",
      "Iteration 21: b-value is 0.06725657505475956\n",
      "Iteration 22: b-value is 0.06726798740674543\n",
      "Iteration 23: b-value is 0.06727598119383463\n"
     ]
    }
   ],
   "source": [
    "# differentiating the error function\n",
    "error_derivative = error_function.diff(b)\n",
    "error_derivative_function = sp.lambdify(b, error_derivative, 'numpy') # Convert the symbolic derivative to a numerical function\n",
    "\n",
    "# Gradient descent algorithm\n",
    "while diff > epsilon and iter_count < max_iter:\n",
    "    b_new = b_value - learning_rate * error_derivative_function(b_value)\n",
    "    diff = abs(b_new - b_value)\n",
    "    b_value = b_new\n",
    "    iter_count += 1\n",
    "    print(f\"Iteration {iter_count}: b-value is {b_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1:\n",
      "Aggression = 0.0673parenting_style\n",
      "\n",
      "Number of iterations is 34\n",
      "The local minimum occurs when b: 0.0673\n",
      "Minimum error: -0.0001\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model 1:\")\n",
    "print(f\"Aggression = {b_value:.4f}parenting_style\")\n",
    "print()\n",
    "print(f\"Number of iterations is {iter_count}\")\n",
    "print(f\"The local minimum occurs when b: {b_value:.4f}\")\n",
    "print(f\"Minimum error: {error_derivative_function(b_value):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./Images/model1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL 2:  SLR with intercept ùíÇ** ($ùíöÃÇùíä =ùíÇ+ùíÉùíôùíä$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_a = 1 # Initial value of a\n",
    "next_b = 1 # Initial value of b\n",
    "alpha = 0.0005 # Learning rate\n",
    "epsilon = 0.00001 # Stopping criterion constant\n",
    "max_iters = 1000 # Maximum number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the symbols \n",
    "a = sp.Symbol('a')\n",
    "b = sp.Symbol('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 666.0 a^{2} - 6.3948846218409 \\cdot 10^{-14} a b + 6.67490040749479 a + 665.0 b^{2} - 89.5019156656474 b + 67.8591267252315$"
      ],
      "text/plain": [
       "666.0*a**2 - 6.3948846218409e-14*a*b + 6.67490040749479*a + 665.0*b**2 - 89.5019156656474*b + 67.8591267252315"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_expr = sum((y_data[i] - (a + b * x_data[i]))**2 for i in range(len(x_data)))\n",
    "func_expr.simplify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$Error Function$$\n",
    "\n",
    "$$\n",
    "E(b) = \\frac{1}{666} \\left[ \\begin{array}{c}\n",
    "(y - (a + bx_1))^2 + \\\\ \n",
    "(y - (a + bx_2))^2 + \\\\ \n",
    "(y - (a + bx_3))^2 + \\\\ \n",
    "\\vdots \\\\\n",
    "(y - (a + bx_{664}))^2 + \\\\ \n",
    "(y - (a + bx_{665}))^2 + \\\\ \n",
    "(y - (a + bx_{666}))^2\n",
    "\\end{array} \\right] \n",
    "$$\n",
    "\n",
    "### $$Substitute$$\n",
    "\n",
    "$$\n",
    "E(b) = \\frac{1}{666} \\left[ \\begin{array}{c}\n",
    "(0.374 - (a + -0.279b))^2 + \\\\ \n",
    "(0.771 - (a + -1.248b))^2 + \\\\ \n",
    "(-0.098 - (a + -0.328b))^2 + \\\\ \n",
    "\\vdots \\\\\n",
    " (-0.062 - (a + -0.335b))^2 +  \\\\ \n",
    " (0.104 - (a + -0.475b))^2 +  \\\\ \n",
    " (-0.316 - (a + -1.102b))^2\n",
    " \\end{array} \\right] \n",
    "$$\n",
    "\n",
    "### $$Simplify$$\n",
    "\n",
    "$$\n",
    "E(a,b) = 666a^2 ‚àí 6.395*10^{‚àí14}ab + 6.675a + 665b^2 ‚àí 89.502b + 67.859\n",
    "$$\n",
    "\n",
    "### $$E'(a,b)$$\n",
    "$$\n",
    "\\frac{‚àÇE}{‚àÇa} = 1332a‚àí5.684*10^{‚àí14}b+6.675\n",
    "$$\n",
    "$$\n",
    "\\frac{‚àÇE}{‚àÇb} = ‚àí5.68*10^{‚àí14}a+1330b‚àí89.502\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 : a =  0.33066254979625254 , b =  0.37975095783282364 , f(a,b) =  204.796926188995\n",
      "Iteration 2 : a =  0.10710384142820084 , b =  0.1719675287068196 , f(a,b) =  80.48838459383433\n",
      "Iteration 3 : a =  0.032435232833271585 , b =  0.10236007994960826 , f(a,b) =  66.5824616222365\n",
      "Iteration 4 : a =  0.007495917562565218 , b =  0.07904158461594246 , f(a,b) =  65.0268451611675\n",
      "Iteration 5 : a =  -0.0008338137378507103 , b =  0.07122988867916442 , f(a,b) =  64.85282117796832\n",
      "Iteration 6 : a =  -0.00361594399218963 , b =  0.06861297054034378 , f(a,b) =  64.83335325830508\n",
      "Iteration 7 : a =  -0.0045451754971388295 , b =  0.06773630296383887 , f(a,b) =  64.8311753787749\n",
      "Iteration 8 : a =  -0.004855538819791862 , b =  0.06744261932570972 , f(a,b) =  64.83093173684605\n",
      "Iteration 9 : a =  -0.004959200169557975 , b =  0.06734423530693646 , f(a,b) =  64.83090448009573\n",
      "Iteration 10 : a =  -0.004993823060379857 , b =  0.06731127666064741 , f(a,b) =  64.83090143079697\n"
     ]
    }
   ],
   "source": [
    "# Calculate partial derivatives\n",
    "partial_a = sp.diff(func_expr, a)\n",
    "partial_b = sp.diff(func_expr, b)\n",
    "\n",
    "# Convert symbolic expressions to numerical functions\n",
    "partialf_a = sp.lambdify((a, b), partial_a)\n",
    "partialf_b = sp.lambdify((a, b), partial_b)\n",
    "func = sp.lambdify((a, b), func_expr)\n",
    "\n",
    "next_func = func(next_a, next_b)                                    # Initial value of function\n",
    "\n",
    "for n in range(max_iters):\n",
    "    current_a = next_a\n",
    "    current_b = next_b\n",
    "    current_func = next_func\n",
    "    next_a = current_a - alpha * partialf_a(current_a, current_b)   # update of a\n",
    "    next_b = current_b - alpha * partialf_b(current_a, current_b)   # update of b\n",
    "    next_func = func(next_a, next_b)\n",
    "    change_func = abs(next_func - current_func)                     # stopping criterion: values of function converge\n",
    "    print(\"Iteration\", n+1, \": a = \", next_a, \", b = \", next_b, \", f(a,b) = \", next_func)\n",
    "    if change_func < epsilon:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2:\n",
      "Aggression = -0.0050 + 0.0673parenting_style\n",
      "\n",
      "Number of iterations is 10\n",
      "The optimal value of intercept is: -0.0050\n",
      "The local minimum occurs when b: 0.0673\n",
      "Minimum error: -0.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model 2:\")\n",
    "print(f\"Aggression = {next_a:.4f} + {next_b:.4f}parenting_style\")\n",
    "print()\n",
    "print(f\"Number of iterations is {n+1}\")\n",
    "print(f\"The optimal value of intercept is: {next_a:.4f}\")\n",
    "print(f\"The local minimum occurs when b: {b_value:.4f}\")\n",
    "print(f\"Minimum error: {error_derivative_function(b_value):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./Images/model2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL 3:  MLR** ($ùíöÃÇùíä=ùíÇ+ùíÉùíôùíä+ùíÑùíòùíä$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Predictors | Variables | \n",
    "| -------- | ------- |\n",
    "| Aggressin | Parenting Style |\n",
    "|           | Sibling Aggression | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y data\n",
    "x_data1 = child_aggression['parenting_style'].values\n",
    "x_data2 = child_aggression['sibling_aggression'].values\n",
    "y_data = child_aggression['aggression'].values\n",
    "\n",
    "# Combine input variables\n",
    "X = np.column_stack((x_data1, x_data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "feature_count = X.shape[1]\n",
    "params = [0,0,0]\n",
    "learning_rate = 0.001\n",
    "convergence_threshold = 0.00001\n",
    "max_iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = sp.symbols('x0:%d' % (feature_count + 1))\n",
    "\n",
    "# Define prediction function (Array with all the data points with symbols)\n",
    "prediction = symbols[0] + sum(symbols[i+1] * X[:, i] for i in range(feature_count))\n",
    "\n",
    "# Define loss function (sum of squared errors)\n",
    "loss_expr = sum((y_data[i] - prediction[i])**2 for i in range(len(y_data)))\n",
    "\n",
    "# Calculate gradients\n",
    "gradients = [sp.diff(loss_expr, symbol) for symbol in symbols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 666.0 x_{0}^{2} - 4.9737991503207 \\cdot 10^{-14} x_{0} x_{1} + 11.0217504158594 x_{0} x_{2} + 6.67490040749478 x_{0} + 665.0 x_{1}^{2} + 75.6163788266224 x_{1} x_{2} - 89.5019156656473 x_{1} + 71.0688739115785 x_{2}^{2} - 17.9002518078758 x_{2} + 67.8591267252315$"
      ],
      "text/plain": [
       "666.0*x0**2 - 4.9737991503207e-14*x0*x1 + 11.0217504158594*x0*x2 + 6.67490040749478*x0 + 665.0*x1**2 + 75.6163788266224*x1*x2 - 89.5019156656473*x1 + 71.0688739115785*x2**2 - 17.9002518078758*x2 + 67.8591267252315"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_expr.simplify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1332.0*x0 - 5.6843418860808e-14*x1 + 11.0217504158594*x2 + 6.67490040749499,\n",
       " -5.6843418860808e-14*x0 + 1330.0*x1 + 75.6163788266224*x2 - 89.5019156656474,\n",
       " 11.0217504158594*x0 + 75.6163788266224*x1 + 142.137747823157*x2 - 17.9002518078758]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$Error Function$$\n",
    "\n",
    "$$\n",
    "E(b) = \\frac{1}{666} \\left[ \\begin{array}{c}\n",
    "(y - (a + bx_1 + cw_1))^2 + \\\\\n",
    "(y - (a + bx_2 + cw_2))^2 + \\\\\n",
    "(y - (a + bx_3 + cw_3))^2 + \\\\\n",
    "\\vdots \\\\\n",
    "(y - (a + bx_{664} + cw_{664}))^2 + \\\\\n",
    "(y - (a + bx_{665} + cw_{665}))^2 + \\\\\n",
    "(y - (a + bx_{666} + cw_{666}))^2\n",
    "\\end{array} \\right] \n",
    "$$\n",
    "\n",
    "### $$Substitute$$\n",
    "\n",
    "$$\n",
    "E(b) = \\frac{1}{666} \\left[ \\begin{array}{c}\n",
    "(0.374 - (a + -0.279b + -0.328c))^2 + \\\\\n",
    "(0.771 - (a + -1.248b + 0.577c))^2 + \\\\\n",
    "(-0.098 - (a + -0.328b + -0.217c))^2 + \\\\\n",
    "\\vdots \\\\\n",
    "(-0.062 - (a + -0.335b + 0.306c))^2 + \\\\\n",
    "(0.104 - (a + -0.475b + 0.094c))^2 + \\\\\n",
    "(-0.316 - (a + -1.102b + 0.149c))^2\n",
    "\\end{array} \\right] \n",
    "$$\n",
    "\n",
    "### $$Simplify$$\n",
    "\n",
    "$$\n",
    "E(a,b,c) = 666a^2‚Äã ‚àí4.974*10^{‚àí14}a‚Äãb‚Äã + 11.022a‚Äãc‚Äã + 6.675a‚Äã +665b^2‚Äã +75.616b‚Äãc‚Äã ‚àí 89.502b‚Äã + 71.069c^2‚Äã ‚àí17.900c‚Äã + 67.859\n",
    "\n",
    "$$\n",
    "### $$E'(a,b,c)$$\n",
    "$$\n",
    "\\frac{‚àÇE}{‚àÇa} = 1332a - 5.684*10^{‚àí14}b + 11.022c + 6.675\n",
    "$$\n",
    "$$\n",
    "\\frac{‚àÇE}{‚àÇb} = -5.684*10^{‚àí14}a + 1330b + 75.616c - 89.502\n",
    "$$\n",
    "$$\n",
    "\\frac{‚àÇE}{‚àÇc} = 11.022a + 75.616b + 142.138c - 17.900\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: params = [-0.00667490040749499, 0.0895019156656474, 0.0179002518078758], loss = 64.98287838184005\n",
      "Iteration 2: params = [-0.004656125580014091, 0.05861273127418749, 0.026561960463972866], loss = 64.57214779570491\n",
      "Iteration 3: params = [-0.005421826013720142, 0.06815119508034398, 0.036305991195687656], loss = 64.46022027080734\n",
      "Iteration 4: params = [-0.005275009744499157, 0.06426669370520476, 0.04395220261096015], loss = 64.39861503599374\n",
      "Iteration 5: params = [-0.005408027379726552, 0.06497040034003501, 0.05080371251154482], loss = 64.35519078077566\n",
      "Iteration 6: params = [-0.005439381156927092, 0.06422009078236407, 0.05662961856536657], loss = 64.32314028944984\n",
      "Iteration 7: params = [-0.00549318338536798, 0.06402715901722136, 0.061684524718925784], loss = 64.29931399154061\n",
      "Iteration 8: params = [-0.005531034959525726, 0.06370859280107789, 0.06603611969253509], loss = 64.28158228377494\n",
      "Iteration 9: params = [-0.005566430430615385, 0.06348466779838079, 0.06979369477144147], loss = 64.26838405424503\n",
      "Iteration 10: params = [-0.005596094188902178, 0.06327442882863477, 0.07303449910923658], loss = 64.25855998323519\n",
      "Iteration 11: params = [-0.0056219651577087745, 0.06309874980014128, 0.07583088727344264], loss = 64.25124743203114\n",
      "Iteration 12: params = [-0.005644197088476728, 0.06294527113277325, 0.07824337247728079], loss = 64.24580432856472\n",
      "Iteration 13: params = [-0.005663405897260425, 0.06281349569791764, 0.08032480300343438], loss = 64.24175274970925\n",
      "Iteration 14: params = [-0.005679969580511452, 0.06269959135225307, 0.0821205597782484], loss = 64.23873695338462\n",
      "Iteration 15: params = [-0.0056942628206517, 0.06260139116175757, 0.08366986732438673], loss = 64.23649214270372\n",
      "Iteration 16: params = [-0.0057065935460160816, 0.06251664419829335, 0.08500654286456234], loss = 64.23482121587317\n",
      "Iteration 17: params = [-0.005717232249385907, 0.06244353613222234, 0.08615977051855822], loss = 64.23357745999787\n",
      "Iteration 18: params = [-0.0057264107872421355, 0.062380458894867874, 0.08715472641544034], loss = 64.2326516692037\n",
      "Iteration 19: params = [-0.00573432966824409, 0.062326039421180444, 0.08801313235778482], loss = 64.23196255600114\n",
      "Iteration 20: params = [-0.005741161735803452, 0.06227908829857395, 0.08875372869623432], loss = 64.23144961397479\n",
      "Iteration 21: params = [-0.005747056157375033, 0.06223858095574827, 0.08939268391430735], loss = 64.23106780512032\n",
      "Iteration 22: params = [-0.00575214161435378, 0.0622036328990577, 0.0899439474621474], loss = 64.23078360535928\n",
      "Iteration 23: params = [-0.00575652913187449, 0.0621734812044988, 0.09041955434697199], loss = 64.23057206101389\n",
      "Iteration 24: params = [-0.005760314496438215, 0.062147467593327795, 0.09082988786041953], loss = 64.23041459779182\n",
      "Iteration 25: params = [-0.00576358034897554, 0.06212502415061612, 0.09118390626882968], loss = 64.23029738990228\n",
      "Iteration 26: params = [-0.0057663979884732645, 0.062105660896629035, 0.09148933838525829], loss = 64.23021014610522\n",
      "Iteration 27: params = [-0.005768828928716285, 0.06208895509982308, 0.09175285230301299], loss = 64.23014520610712\n",
      "Iteration 28: params = [-0.0057709262411882, 0.06207454204453802, 0.09198020097105448], loss = 64.23009686796159\n",
      "Iteration 29: params = [-0.005772735713724055, 0.062062107069773734, 0.09217634779055306], loss = 64.23006088741997\n",
      "Iteration 30: params = [-0.0057742968501315285, 0.062051378699237124, 0.0923455749742026], loss = 64.23003410527177\n",
      "Iteration 31: params = [-0.005775643732626012, 0.0620421227146876, 0.09249157703408425], loss = 64.23001416995584\n",
      "Iteration 32: params = [-0.00577680576590206, 0.062034137042519436, 0.0926175414390336], loss = 64.22999933108568\n",
      "Iteration 33: params = [-0.0057778083190870464, 0.062027247342171625, 0.09272621820241016], loss = 64.22998828575945\n",
      "Iteration 34: params = [-0.00577867327959157, 0.06202130319997726, 0.09281997991948214], loss = 64.22998006416107\n",
      "Final parameters: [-0.00577867327959157, 0.06202130319997726, 0.09281997991948214]\n"
     ]
    }
   ],
   "source": [
    "# Convert symbolic expressions to numerical functions\n",
    "grad_funcs = [sp.lambdify(symbols, gradient) for gradient in gradients]\n",
    "loss_func = sp.lambdify(symbols, loss_expr)\n",
    "\n",
    "current_loss = loss_func(*params)  # Initial loss value\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    old_params = params.copy()\n",
    "    old_loss = current_loss\n",
    "    \n",
    "    # Update parameters\n",
    "    for i in range(len(params)):\n",
    "        params[i] = old_params[i] - learning_rate * grad_funcs[i](*old_params)\n",
    "    \n",
    "    current_loss = loss_func(*params)\n",
    "    loss_change = abs(current_loss - old_loss)\n",
    "    print(f\"Iteration {iteration+1}: params = {params}, loss = {current_loss}\")\n",
    "    \n",
    "    if loss_change < convergence_threshold:\n",
    "        break\n",
    "\n",
    "print(\"Final parameters:\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model3:\n",
      "Aggression = -0.0058 + 0.0620parenting_style + 0.0928sibling_aggression\n",
      "\n",
      "Number of iterations is 34\n",
      "Intercept = -0.0058\n",
      "Coefficient for parenting_style = 0.0620\n",
      "Coefficient for sibling_aggression = 0.0928\n",
      "Minimum error: 64.2300\n"
     ]
    }
   ],
   "source": [
    "intercept, coef1, coef2 = params\n",
    "\n",
    "print(f\"Model3:\")\n",
    "print(f\"Aggression = {intercept:.4f} + {coef1:.4f}parenting_style + {coef2:.4f}sibling_aggression\")\n",
    "print()\n",
    "print(f\"Number of iterations is {iteration+1}\")\n",
    "print(f\"Intercept = {intercept:.4f}\")\n",
    "print(f\"Coefficient for parenting_style = {coef1:.4f}\")\n",
    "print(f\"Coefficient for sibling_aggression = {coef2:.4f}\")\n",
    "print(f\"Minimum error: {loss_func(intercept, coef1, coef2):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./Images/model3.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
